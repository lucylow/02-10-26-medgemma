# MedGemma-4B LoRA/QLoRA recommended configuration
# Proven in MedGemma fine-tunes for endoscopy VQA, MRI, and pediatric tasks

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: "all-linear"  # or ["q_proj","k_proj","v_proj","o_proj"] for VRAM limits
  modules_to_save: ["lm_head", "embed_tokens"]

quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true

training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8  # effective batch 16
  learning_rate: 1.0e-4
  optimizer: "adamw_torch"
  num_train_epochs: 3
  bf16: true

# When to adjust:
# - Very small dataset (100–500): lora_dropout → 0.1
# - Large dataset (>5k–10k): dropout 0.0–0.03; consider r=32 if underfitting
# - VRAM limits: target_modules → ["q_proj","k_proj","v_proj","o_proj"]
