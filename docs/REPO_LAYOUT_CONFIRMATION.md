# Repo Layout Confirmation â€” PediScreen AI / MedGemma Impact Challenge

**Date:** 2026-02-14  
**Purpose:** Confirm actual repository structure vs. the governing instruction set before implementing the 15+ page file-by-file improvements.

---

## âœ… Confirmed Components

| Your Assumption | Actual Location | Notes |
|-----------------|-----------------|-------|
| **backend/** | `backend/` | FastAPI service with `/api/analyze`, Mongo/CloudSQL persistence |
| **model_wrapper** | `backend/app/services/model_wrapper.py` | Deterministic baseline + HF Inference API; already uses `analyze()` as primary risk logic |
| **MedGemma integration** | `backend/app/services/medgemma_service.py` | `MedGemmaService` â€” Vertex AI / Hugging Face; embedded in backend, not a separate server |
| **orchestrator/** | `pedi-agent-stack/orchestrator/` | Multi-agent MVP: embedding, Celery worker, `process_case` â†’ MedGemma pipeline |
| **client/frontend** | `app/frontend/` | React (Vite) â€” clinician and parent views; also `src/` for landing/demo |
| **EvidenceItem / Report** | `backend/app/models/schemas.py` | `EvidenceItem(type, content, influence)` â€” needs extension per spec |

---

## âš ï¸ Structural Differences (Adjustments Required)

### 1. No Dedicated MedGemma Server

- **Your assumption:** `medgemma-server/` or similar â€” a FastAPI service wrapping MedGemma (HF AutoModelForCausalLM or Vertex).
- **Reality:** MedGemma inference is **embedded** in the backend:
  - `backend/app/services/medgemma_service.py` â€” `MedGemmaService` class
  - `backend/app/api/infer.py` â€” `/api/infer` endpoint (precomputed embedding)
  - `backend/app/api/analyze.py` â€” `/api/analyze` calls `MedGemmaService.analyze_input()` when HF/Vertex configured

**Implication:** Part 1 of your plan can be implemented in two ways:
- **Option A:** Extract `MedGemmaService` into a new `medgemma-server/` microservice with `/infer` and strict JSON contract (matches your spec).
- **Option B:** Harden `MedGemmaService` in-place within `backend/` and expose a well-defined `/api/infer` contract; orchestrator/backend call it via HTTP or in-process.

### 2. Orchestrator MedGemma Target

- **pedi-agent-stack** uses `MEDGEMMA_URL = http://medgemma-llm:8000/infer`
- **medgemma-llm** in the stack is a **demo** service using `google/flan-t5-small`, not MedGemma
- The **main backend** (`backend/app/api/infer.py`) serves real MedGemma inference when configured

**Implication:** Orchestrator should call either:
- The new/refactored MedGemma server (if Option A), or
- `backend`â€™s `/api/infer` endpoint (if Option B)

### 3. Dual Backend Paths

- `backend/` â€” primary FastAPI app (analyze, infer, reports, etc.)
- `app/backend/` â€” alternate structure (Lovable/Supabase scaffold) with its own `main.py` and `/v1/process_case`

**Implication:** Focus improvements on `backend/` as the canonical service; `app/backend/` can be aligned later if needed.

### 4. Agent System Variants

- `pedi-agent-stack/orchestrator/` â€” Celery-based, calls embed-server + medgemma-llm
- `app/backend/orchestrator/` â€” integrated orchestrator (job_store, tasks, webhooks)
- `agent_system/` â€” `CentralOrchestrator` with `run_workflow`

**Implication:** Part 3 improvements should target `pedi-agent-stack/orchestrator/` as the primary multi-agent MVP; schemas can be shared via a common `orchestrator/schemas.py` or imported from backend.

---

## ğŸ“ Actual Directory Map

```
02-10-26-medgemma/
â”œâ”€â”€ backend/                    # Primary FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.py      # POST /api/analyze
â”‚   â”‚   â”‚   â”œâ”€â”€ infer.py        # MedGemma /api/infer
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ medgemma_service.py   # MedGemmaService (Vertex/HF)
â”‚   â”‚   â”‚   â”œâ”€â”€ model_wrapper.py      # Deterministic + HF
â”‚   â”‚   â”‚   â”œâ”€â”€ medsiglip_*.py        # Embedding services
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ models/schemas.py   # EvidenceItem, Report, AnalyzeResponse
â”‚   â”‚   â””â”€â”€ core/config.py      # settings
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ backend/                # Alternate backend (Lovable)
â”‚   â”‚   â”œâ”€â”€ main.py             # /v1/process_case
â”‚   â”‚   â””â”€â”€ orchestrator/
â”‚   â””â”€â”€ frontend/               # React (Vite) â€” clinician/parent UI
â”œâ”€â”€ pedi-agent-stack/
â”‚   â”œâ”€â”€ orchestrator/           # Multi-agent MVP
â”‚   â”‚   â”œâ”€â”€ main.py             # POST /v1/process_case
â”‚   â”‚   â”œâ”€â”€ tasks.py            # Celery: run_medgemma_pipeline
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ job_store.py, webhooks.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ medgemma-llm/           # Demo (flan-t5-small)
â”‚   â”œâ”€â”€ embed-server/           # Mock MedSigLIP
â”‚   â””â”€â”€ clinician-ui/           # React clinician UI
â”œâ”€â”€ pedi_screen/                # CLI package
â”‚   â””â”€â”€ medgemma_core/          # Inference engine, model_loader
â”œâ”€â”€ model/                      # Model card, finetuning, evaluation
â”œâ”€â”€ src/                        # React landing/demo
â”œâ”€â”€ frontend/                   # UX guide, design tokens
â”œâ”€â”€ docs/
â””â”€â”€ configs/inference.yaml
```

---

## ğŸ”§ Recommended Path Forward

1. **Part 1 â€” MedGemma Server Hardening**
   - Add `backend/app/services/medgemma_config.py` (or extend `core/config.py`) with `MEDGEMMA_*` env vars.
   - Introduce `MedGemmaInferenceRequest` / `MedGemmaInferenceResponse` Pydantic schemas in `backend/app/schemas/` or `models/`.
   - Refactor `MedGemmaService` to:
     - Use strict JSON-only prompt + parsing.
     - Support adapter loading via new `adapter_manager.py`.
     - Return schema-aligned responses with `adapter_id`, `model_name`, `schema_version`.
   - Add `/api/infer` (or `/infer`) route that accepts only the strict schema.
   - Optionally extract to `medgemma-server/` later for microservice deployment.

2. **Part 2 â€” Backend Model Wrapper**
   - Keep `model_wrapper.py` as primary deterministic logic (never bypassed).
   - When MedGemma is used: call the hardened `/api/infer` (or in-process `MedGemmaService`) and append as `EvidenceItem` with `type="model_text"`, `influenceâ‰ˆ0.2â€“0.3`, `source_model`, `adapter_id`.
   - Extend `EvidenceItem` schema per your spec.
   - Add safety filter (banned phrases, over-confident language) before surfacing model output.

3. **Part 3 â€” Orchestrator**
   - Update `pedi-agent-stack/orchestrator/` to use the new MedGemma contract.
   - Add `orchestrator/schemas.py` with `EmbeddingItem`, `TemporalOutput`, `MedGemmaOutput`, `SafetyAgent` types.
   - Implement `SafetyAgent` with banned phrase list and clear ok/blocked behavior.
   - Ensure `process_case` is exception-safe and returns degraded responses on agent failure.

4. **Part 4 â€” Client**
   - Differentiate clinician vs. caregiver views (already partially present).
   - Add â€œAI insights updatedâ€ indicator for async MedGemma results.
   - Preserve offline-first patterns.

5. **Part 5 â€” Docs**
   - Update README sections for MedGemma config, JSON schema, adapters.
   - Add â€œSafety by designâ€ and model/adapter card.

---

## Confirmation

If this layout matches your understanding and the recommended path is acceptable, you can proceed with the expanded file-by-file implementation. Any path or naming adjustments can be made during implementation.
