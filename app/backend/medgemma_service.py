"""
MedGemmaService
- loads MedGemma base model
- optionally attaches LoRA adapters (PEFT)
- exposes `infer` that accepts precomputed embeddings or raw images (hook)
"""

from typing import Optional, Union, Dict, Any
import os
import time
import json
import re
import numpy as np
import torch
from loguru import logger
from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Environment-driven defaults
MODEL_NAME = os.getenv("MEDGEMMA_MODEL_NAME", "google/medgemma-2b-it")
ADAPTER_LOCAL_DIR = os.getenv("ADAPTER_LOCAL_DIR", "/app/adapters")
DEVICE_AUTO = os.getenv("DEVICE_AUTO", "1") == "1"


class MedGemmaService:
    def __init__(
        self,
        model_name: str = MODEL_NAME,
        adapter_dir: Optional[str] = None,
        device: Optional[str] = None
    ):
        self.model_name = model_name
        self.adapter_dir = adapter_dir or ADAPTER_LOCAL_DIR
        
        # choose device
        if device:
            self.device = torch.device(device)
        else:
            self.device = torch.device(
                "cuda" if (torch.cuda.is_available() and DEVICE_AUTO) else "cpu"
            )
        logger.info("MedGemmaService device: {}", self.device)
        
        # load components
        self.tokenizer = None
        self.processor = None
        self.model = None
        try:
            self._load_base_model()
        except Exception as e:
            logger.critical("Failed to load base model during initialization: {}", e)
            # We don't reraise here to allow the service object to exist, 
            # but health checks will report failure.

        # attempt to attach adapter if present
        if os.path.exists(self.adapter_dir) and os.listdir(self.adapter_dir):
            try:
                self.attach_adapter(self.adapter_dir)
            except Exception as e:
                logger.warning("Failed to attach adapter at init: {}", e)

    def _load_base_model(self):
        logger.info("Loading base model: {}", self.model_name)
        
        # Load tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, trust_remote_code=True
            )
        except Exception as e:
            logger.warning("Tokenizer not available or failed: {}", e)
            self.tokenizer = None

        # Load processor
        try:
            self.processor = AutoProcessor.from_pretrained(
                self.model_name, trust_remote_code=True
            )
        except Exception as e:
            logger.debug("Processor not available: {}", e)
            self.processor = None

        # Load model with device_map="auto" for large models
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name, trust_remote_code=True, device_map="auto"
            )
            logger.info("Loaded model with device_map=auto")
        except Exception as e:
            logger.warning(
                "device_map=auto failed: {}. Loading to single device.", e
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name, trust_remote_code=True
            )
            self.model.to(self.device)
        
        self.model.eval()

    def attach_adapter(self, adapter_path: str):
        """
        Attach a LoRA adapter saved via Peft.
        adapter_path may be local path or a HF repo identifier.
        
        Note: We use LoRA (Low-Rank Adaptation) to specifically fine-tune MedGemma 
        on developmental milestones while preserving its broad medical foundation 
        knowledge. This allows for parameter-efficient adaptation to pediatric 
        nuances without forgetting general clinical reasoning. This approach is 
        aligned with the 2025 Stanford study finding that domain-specific 
        fine-tuning significantly improves diagnostic performance.
        """
        if not adapter_path:
            raise ValueError("adapter_path required")
        
        logger.info("Attaching adapter from: {}", adapter_path)
        try:
            self.model = PeftModel.from_pretrained(
                self.model, adapter_path, device_map="auto"
            )
            logger.info("Adapter attached from %s", adapter_path)
            try:
                self.model.print_trainable_parameters()
            except Exception:
                pass
        except Exception as e:
            logger.exception("Failed to attach adapter: %s", e)
            raise

    def reload_adapter(self, adapter_path: str):
        """
        Replace current adapter with a different one.
        """
        logger.info("Reloading adapter: %s", adapter_path)
        self._load_base_model()
        self.attach_adapter(adapter_path)

    def _normalize_embedding(
        self, emb: Union[np.ndarray, torch.Tensor]
    ) -> torch.Tensor:
        if isinstance(emb, np.ndarray):
            emb = torch.from_numpy(emb)
        if not isinstance(emb, torch.Tensor):
            raise TypeError("Embedding must be numpy or torch Tensor")
        
        t = emb
        if t.dim() == 1:
            t = t.unsqueeze(0)
        if t.dim() == 2:
            t = t.unsqueeze(1)
        
        t = t.to(self.device).to(torch.float32)
        t = torch.nn.functional.normalize(t, dim=-1)
        return t

    def build_prompt(
        self,
        age_months: int,
        observations: str,
        domain: str = "",
        domain_prompt: Optional[str] = None,
        questionnaire_scores: Optional[Dict[str, Any]] = None,
        visual_evidence: Optional[str] = None
    ) -> str:
        """
        Build an enhanced clinical prompt for structured report generation.
        Uses evidence-grounded summarization approach.
        """
        if age_months < 0:
            raise ValueError("age_months cannot be negative")

        domain_labels = {
            "communication": "Communication & Language",
            "gross_motor": "Gross Motor Skills",
            "fine_motor": "Fine Motor Skills",
            "cognitive": "Problem Solving / Cognitive",
            "social": "Personal-Social",
        }
        domain_label = domain_labels.get(domain, "General Development")
        
        # Build context sections
        context_sections = []
        
        # Core screening data
        context_sections.append(f"- Child age (months): {age_months}")
        context_sections.append(f"- Developmental domain: {domain_label}")
        
        # Visual evidence description (from MedSigLIP analysis)
        if visual_evidence:
            context_sections.append(f"- Clinical Vision Findings (MedSigLIP): {visual_evidence}")
        
        # Parent/caregiver observations + Questionnaire highlights
        highlights = []
        if observations:
            highlights.append(observations)
        if questionnaire_scores:
            for k, v in questionnaire_scores.items():
                if v is not None:
                    highlights.append(f"{k}: {v}")
        
        if highlights:
            context_sections.append("- Screening Evidence:")
            for h in highlights:
                context_sections.append(f"  - {h}")
        
        # Reference impact data for model context
        context_sections.append("- Public Health Context: Early identification before age 3 can save $30k-$100k in lifetime costs per child and improve long-term outcomes in language and social integration.")
        
        input_context = "\n".join(context_sections)
        
        prompt = f"""SYSTEM:
You are a pediatric developmental screening support model.
You do not diagnose.
You summarize screening observations for clinician review.

INPUT CONTEXT:
{input_context}

TASKS:
1. Generate a clinical screening summary (3â€“4 bullets)
2. Assign a screening risk level (Low / Moderate / Elevated)
3. Provide rationale for the risk level
4. Provide differential screening considerations (reasoned enumeration of factors)
5. Suggest actionable next screening steps
6. Estimate economic impact of early intervention for this specific case based on the identified risk.

OUTPUT FORMAT:
JSON with keys:
"risk_stratification", "clinical_summary", "parent_friendly_explanation", "differential_considerations", "supporting_evidence", "developmental_profile", "recommendations", "referral_guidance", "follow_up", "economic_impact"

Detailed JSON Schema:
{{
    "risk_stratification": {{
        "level": "low" | "moderate" | "elevated",
        "primary_domain": "{domain_label}",
        "confidence": 0.0-1.0,
        "rationale": "Explanation of risk determination"
    }},
    "clinical_summary": "3-4 bullet clinical summary",
    "differential_considerations": ["Consideration 1", "Consideration 2"],
    "supporting_evidence": {{
        "from_parent_report": [],
        "from_assessment_scores": [],
        "from_visual_analysis": []
    }},
    "developmental_profile": {{
        "strengths": [],
        "concerns": [],
        "milestones_met": [],
        "milestones_emerging": [],
        "milestones_not_observed": []
    }},
    "recommendations": {{
        "immediate": [],
        "short_term": [],
        "long_term": [],
        "parent_friendly_tips": []
    }},
    "referral_guidance": {{
        "needed": true | false,
        "urgency": "routine" | "priority" | "urgent",
        "specialties": [],
        "reason": ""
    }},
    "follow_up": {{
        "rescreen_interval_days": 90,
        "monitoring_focus": [],
        "red_flags_to_watch": []
    }},
    "economic_impact": {{
        "early_intervention_value": "Estimated savings (e.g. '$30,000 to $100,000')",
        "description": "Societal and lifetime benefit explanation based on early identification."
    }}
}}

IMPORTANT: 
- You are a junior clinical reasoning assistant, not a final diagnostician.
- Every finding must be grounded in the input data.
- Use developmentally appropriate language.
- Respond with ONLY the valid JSON object."""

        if domain_prompt:
            prompt = domain_prompt + "\n\n" + prompt
        
        return prompt

    def infer(
        self,
        precomputed_image_emb: Optional[Union[np.ndarray, torch.Tensor]] = None,
        age_months: int = 24,
        observations: str = "",
        domain: str = "",
        max_new_tokens: int = 1024,
        temperature: float = 0.1,
        return_raw: bool = False,
        questionnaire_scores: Optional[Dict[str, Any]] = None,
        visual_evidence: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Run inference with enhanced multimodal reasoning.
        Supports precomputed embeddings, questionnaire data, and visual evidence.
        Returns a dict with generated text, structured JSON, and metadata.
        
        Note: The generated outputs are designed as Clinical Decision Support (CDS) 
        signals. MedGemma provides observations and risk stratification intended 
        for clinician review, NOT autonomous diagnosis. This design follows 
        FDA Enforcement Discretion guidelines for non-device CDS.
        """
        if self.model is None:
            logger.error("MedGemma model not loaded")
            raise RuntimeError("MedGemma model not loaded. Please check logs for initialization errors.")

        try:
            prompt = self.build_prompt(
                age_months=age_months,
                observations=observations,
                domain=domain,
                questionnaire_scores=questionnaire_scores,
                visual_evidence=visual_evidence
            )
        except Exception as e:
            logger.error("Failed to build prompt: {}", e)
            raise ValueError(f"Failed to build clinical prompt: {str(e)}")
        
        if self.tokenizer is None:
            logger.error("Model tokenizer not loaded")
            raise RuntimeError("Model tokenizer not loaded.")
        
        try:
            text_inputs = self.tokenizer(
                prompt, return_tensors="pt", padding=True
            ).to(self.device)
        except Exception as e:
            logger.error("Tokenization failed: {}", e)
            raise RuntimeError(f"Tokenization failed: {str(e)}")
        
        model_inputs = dict(text_inputs)

        if precomputed_image_emb is not None:
            try:
                emb_t = self._normalize_embedding(precomputed_image_emb)
                # Attempt to inject embedding into model inputs. 
                # Different model versions might use different keys for image embeddings.
                injection_success = False
                
                # Check for standard MedGemma/SigLIP expected keys
                potential_keys = ("image_embeds", "vision_embeds", "image_features", "image_embed", "inputs_embeds")
                
                for key in potential_keys:
                    try:
                        # Some models might not support direct injection this way if not wrapped correctly
                        model_inputs[key] = emb_t
                        logger.debug("Injected embedding under key: %s", key)
                        injection_success = True
                        break
                    except Exception:
                        continue
                
                # Special handling for models that expect vision embeddings to be merged with text embeddings
                # if "inputs_embeds" was NOT the successful key but we have text input_ids
                if not injection_success and "input_ids" in model_inputs:
                    try:
                        # This is a common pattern for multimodal LLMs where image tokens are replaced by embeddings
                        # Here we assume a simplified version or a specific placeholder
                        logger.debug("Attempting manual embedding concatenation (advanced)")
                        # ... implementation-specific logic would go here ...
                        pass
                    except Exception:
                        pass

                if not injection_success:
                    logger.warning("Could not identify correct key for image embedding injection. Model may ignore visual data.")
            except Exception as e:
                logger.error("Embedding normalization/injection failed: {}", e)
                # We can continue with text-only if embedding fails, but let's log it.

        start = time.time()
        fallback = False
        
        try:
            with torch.no_grad():
                out_ids = self.model.generate(
                    **model_inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0
                )
        except Exception as e:
            logger.warning(
                "Model generate with multimodal inputs failed: {}. Falling back to text-only.",
                e
            )
            try:
                with torch.no_grad():
                    out_ids = self.model.generate(
                        input_ids=text_inputs["input_ids"].to(self.device),
                        attention_mask=text_inputs.get("attention_mask"),
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        do_sample=temperature > 0
                    )
                fallback = True
            except Exception as e_inner:
                logger.exception("Text-only fallback also failed")
                raise RuntimeError(f"Model generation failed entirely: {str(e_inner)}")
        
        elapsed = time.time() - start

        try:
            generated_text = self.tokenizer.decode(
                out_ids[0], skip_special_tokens=True
            )
        except Exception as e:
            logger.warning("Tokenizer decode failed: {}", e)
            try:
                if self.processor:
                    generated_text = self.processor.decode(
                        out_ids[0], skip_special_tokens=True
                    )
                else:
                    generated_text = str(out_ids[0])
            except Exception:
                generated_text = str(out_ids[0])

        result = {
            "text": generated_text,
            "inference_time_s": elapsed,
            "fallback_to_text_only": fallback,
            "model": self.model_name,
            "adapter_path": self.adapter_dir,
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }
        
        if return_raw:
            return result
        
        try:
            parsed = self._safe_parse_json_from_text(generated_text)
            if parsed is not None:
                result["json"] = parsed
            else:
                logger.warning("Model output could not be parsed as JSON: {}", generated_text[:100] + "...")
        except Exception as e:
            logger.error("JSON parsing logic failed: {}", e)
        
        return result

    def _safe_parse_json_from_text(self, text: str) -> Optional[Dict[str, Any]]:
        """
        Best-effort extraction of a JSON object from model text output.
        Handles markdown code blocks and common LLM malformations.
        """
        if not text:
            return None

        # Try to find JSON in markdown code blocks first
        json_pattern = re.compile(r"```(?:json)?\s*(\{.*?\})\s*```", re.DOTALL)
        match = json_pattern.search(text)
        if match:
            text = match.group(1)

        start = text.find("{")
        end = text.rfind("}")
        
        if start != -1 and end != -1 and end > start:
            maybe = text[start:end + 1]
            # Basic cleanup
            maybe = maybe.replace("\n", " ").strip()
            
            try:
                return json.loads(maybe)
            except Exception:
                try:
                    # More aggressive cleanup for common LLM JSON errors
                    # Remove trailing commas in objects and arrays
                    cleaned = re.sub(r",\s*}", "}", maybe)
                    cleaned = re.sub(r",\s*]", "]", cleaned)
                    # Handle unescaped newlines in strings (already handled by replace above, but for completeness)
                    return json.loads(cleaned)
                except Exception as e:
                    logger.debug("Failed to parse cleaned JSON: {}", e)
                    return None
        return None
