# PediScreen LoRA training config (runbook Page 7)
base_model: google/medgemma-2b-it
adapter_out_dir: outputs/adapters/pediscreen_v1

train:
  epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  fp16: true
  dataloader_num_workers: 0
  logging_steps: 10
  save_strategy: epoch
  eval_strategy: steps
  eval_steps: 500
  load_best_model_at_end: true
  metric_for_best_model: eval_loss

peft:
  r: 8
  alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM

evaluation:
  eval_steps: 500
  early_stop_patience: 5

data:
  train_path: data/manifests/train.jsonl
  dev_path: data/manifests/dev.jsonl
  text_column: text
  label_column: reference_flag  # on_track | monitor | refer
  max_seq_length: 512
  sample_weight_column: null  # optional: weight for rare classes
